#In her subgroup, all others have normal Thalessemia.
#Replace NA with normal for this case.
data1[Thal %in% dt[Sex == "female",is.na(Thal)],  Thal:= "normal"]
#In her subgroup, all others have normal Thalessemia.
#Replace NA with normal for this case.
data1[Sex == "female" & is.na(Thal), Thal:= "normal"]
summary(data1$Thal)
library(data.table)
library(caTools)
setwd("~/Desktop/AY2020sem1 CBA")
data1 <- fread("AHD.csv", stringsAsFactors=T, header = T)
data1$Sex <- factor(data1$Sex)
data1$ChestPain <- factor(data1$ChestPain)
data1$Fbs <- factor(data1$Fbs)
data1$RestECG <- factor(data1$RestECG)
data1$ExAng <- factor(data1$ExAng)
data1$Slope <- factor(data1$Slope)
#Need to check if Ca is categorical
data1$Ca <- factor(data1$Ca)
data1$Thal <- factor(data1$Thal)
data1$AHD <- factor(data1$AHD)
#Cleaning NAs in  Ca
testCadata <- data1[!is.na(Ca)] #4 rows with NA values are temporarily removed and stored into another dataset
# Create the function. for mode because R doesn't have a built in function for mode
mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
mode(testCadata$Ca) #The most common value in the dataset is 0, we can use this to replace the NA values
data1[, FinalCa := Ca]  # create a copy of Cleaned Column column.
data1[is.na(Ca), FinalCa:= mode(testCadata$Ca)]
#Verify that the values have been correctly replaced in the dataset
#data1[is.na(testCa), .N, by = .(employment, Ca)]
#table(data1$testCa, custdata.dt$Ca, useNA = 'ifany', deparse.level = 2)
sapply(data1, function(x) sum(is.na(x))) #0 NAs remaining in Final Ca
#not sure pls check if need to include this or not
levels(data1$AHD) # Baseline is Default = "No"
m1 <- glm(AHD ~ . , family = binomial, data = data1)
summary(m1)
#Cleaning NAs in  Ca
testCadata <- data1[!is.na(Ca)] #4 rows with NA values are temporarily removed and stored into another dataset
# Create the function. for mode because R doesn't have a built in function for mode
mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
mode(testCadata$Ca) #The most common value in the dataset is 0, we can use this to replace the NA values
data1[, FinalCa := Ca]  # create a copy of Cleaned Column column.
data1[is.na(Ca), FinalCa:= mode(testCadata$Ca)]
#Verify that the values have been correctly replaced in the dataset
#data1[is.na(testCa), .N, by = .(employment, Ca)]
#table(data1$testCa, custdata.dt$Ca, useNA = 'ifany', deparse.level = 2)
sapply(data1, function(x) sum(is.na(x))) #0 NAs remaining in Final Ca
#not sure pls check if need to include this or not
levels(data1$AHD) # Baseline is Default = "No"
#Verify that the values have been correctly replaced in the dataset
#data1[is.na(testCa), .N, by = .(employment, Ca)]
#table(data1$testCa, custdata.dt$Ca, useNA = 'ifany', deparse.level = 2)
sapply(data1, function(x) sum(is.na(x))) #0 NAs remaining in Final Ca
data1[is.na(Thal), Thal:= mode(Thal)]
summary(data1$Thal)
mode(Thal)
data1[is.na(Thal), Thal:= mode(data1$Thal)]
summary(data1$Thal)
mode(Thal)
summary(data1$Thal)
data1[is.na(Thal), Thal:= mode(data1$Thal)]
mode(data1$Thal)
summary(data1$Thal)
data1[is.na(Thal), Thal:= mode(data1$Thal)]
summary(data1$Thal)
data1[is.na(Thal), Thal:= mode(data1$Thal)]
library(data.table)
library(caTools)
setwd("~/Desktop/AY2020sem1 CBA")
data1 <- fread("AHD.csv", stringsAsFactors=T, header = T)
data1$Sex <- factor(data1$Sex)
data1$ChestPain <- factor(data1$ChestPain)
data1$Fbs <- factor(data1$Fbs)
data1$RestECG <- factor(data1$RestECG)
data1$ExAng <- factor(data1$ExAng)
data1$Slope <- factor(data1$Slope)
#Need to check if Ca is categorical
data1$Ca <- factor(data1$Ca)
data1$Thal <- factor(data1$Thal)
data1$AHD <- factor(data1$AHD)
#Cleaning NAs in  Ca
testCadata <- data1[!is.na(Ca)] #4 rows with NA values are temporarily removed and stored into another dataset
# Create the function. for mode because R doesn't have a built in function for mode
mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
mode(testCadata$Ca) #The most common value in the dataset is 0, we can use this to replace the NA values
data1[, FinalCa := Ca]  # create a copy of Cleaned Column column.
data1[is.na(Ca), FinalCa:= mode(testCadata$Ca)]
#Verify that the values have been correctly replaced in the dataset
#data1[is.na(testCa), .N, by = .(employment, Ca)]
#table(data1$testCa, custdata.dt$Ca, useNA = 'ifany', deparse.level = 2)
sapply(data1, function(x) sum(is.na(x))) #0 NAs remaining in Final Ca
data1[is.na(Thal), Thal:= mode(data1$Thal)]
#not sure pls check if need to include this or not
levels(data1$AHD) # Baseline is Default = "No"
m1 <- glm(AHD ~ . , family = binomial, data = data1)
summary(m1)
#Age, ChestPainnontypical, Chol, Fbs, RestECG, ExAng, Oldpeak, Slope, Thalnormal, Thalreversable not significant in the presence of all other variables
#However, we should not drop levels belonging to ChestPain, Slope and Thal and keep them in the next model
is.na(data1) <- sapply(data1, is.infinite)
data1[is.na(data1)] <- 0
data1[is.nan(data1)] <- 0
m2 <- glm(AHD ~ .  -RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(m2)
OR.m2 <- exp(coef(m2))
OR.m2
# Output the probability from the logistic function for all cases in the data.
prob <- predict(m2, type = 'response')
# If Threshold = 0.5 ---------------------------------------------
threshold1 <- 0.5
m2.predict <- ifelse(prob > threshold1, "Yes", "No")
table1 <- table(Actual = data1$AHD, m2.predict, deparse.level = 2)
table1
round(prop.table(table1),3)
# Overall Accuracy
mean(m2.predict == data1$AHD)
#Logistic Regression
m1 <- glm(AHD ~ . , family = binomial, data = data1)
summary(m1)
data1$FinalCa = factor(data1$FinalCa)
#not sure pls check if need to include this or not
levels(data1$AHD) # Baseline is Default = "No"
#Logistic Regression
m1 <- glm(AHD ~ . , family = binomial, data = data1)
summary(m1)
View(data1)
#Logistic Regression
m1 <- glm(AHD ~ . , family = binomial, data = data1)
summary(m1)
#Logistic Regression
m1 <- glm(AHD ~ . -Ca , family = binomial, data = data1)
summary(m1)
#Age, ChestPainnontypical, Chol, Fbs, RestECG, ExAng, Oldpeak, Slope, Thalnormal, Thalreversable not significant in the presence of all other variables
#However, we should not drop levels belonging to ChestPain, Slope and Thal and keep them in the next model
is.na(data1) <- sapply(data1, is.infinite)
data1[is.na(data1)] <- 0
data1[is.nan(data1)] <- 0
#Logistic Regression
m1 <- glm(AHD ~ . -Ca , family = binomial, data = data1)
summary(m1)
m2 <- glm(AHD ~ . -Ca-RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(m2)
m2 <- glm(AHD ~ . -Ca-RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(m2)
OR.m2 <- exp(coef(m2))
# Output the probability from the logistic function for all cases in the data.
prob <- predict(m2, type = 'response')
prob
m2.predict <- ifelse(prob > threshold1, "Yes", "No")
table1 <- table(Actual = data1$AHD, m2.predict, deparse.level = 2)
table1
round(prop.table(table1),3)
# Overall Accuracy
mean(m2.predict == data1$AHD)
#Train Test Split
#70% trainset
train <- sample.split(Y = data1$AHD, SplitRatio = 0.7)
trainset <- subset(data1, train == T)
testset <- subset(data1, train == F)
# Checking the distribution of Y is similar in trainset vs testset.
summary(trainset$AHD)
summary(testset$AHD)
# Develop model on trainset for AHD
AHD_model <- glm(AHD ~ . -Ca -RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(corpbonds_model)
# Develop model on trainset for AHD
AHD_model <- glm(AHD ~ . -Ca -RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(AHD_model)
residuals(AHD_model)
?residuals
plot(AHD_model,which=1)
plot(AHD_model,which=1)
plot(predict(AHD_model),residuals(AHD_model))
abline(h=0,lty=2,col="dark blue")
plot(predict(AHD_model),residuals(AHD_model))
abline(h=0,lty=2,col="dark blue")
plot(AHD_model,which=1)
plot(predict(AHD_model),residuals(AHD_model), col=c("blue","red")[1+Y])
abline(h=0,lty=2,col="dark blue")
plot(predict(AHD_model),residuals(AHD_model), col=c("blue","red")[1+Y])
residuals(AHD_model)
summary(m2)
OR.m2 <- exp(coef(m2))
# Develop model on trainset for AHD
AHD_model <- glm(AHD ~ . -Ca -RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(AHD_model)
residuals(AHD_model)
RMSE.AHD_model.train <- sqrt(mean(residuals(AHD_model)^2))
summary(abs(residuals(AHD_model)))  # Check Min Abs Error and Max Abs Error.
# Apply model from trainset to predict on testset.
predict.AHD_model.test <- predict(AHD_model, newdata = testset)
AHD_model.testset.error <- testset$AHD - predict.AHD_model.test
# Apply model from trainset to predict on testset.
predict.AHD_model.test <- predict(AHD_model, newdata = testset)
AHD_model.testset.error <- testset$AHD - predict.AHD_model.test
AHD_model.testset.error
# Confusion Matrix on Trainset
prob.train <- predict(m2, type = 'response')
m2.predict.train <- ifelse(prob.train > threshold1, "Yes", "No")
table1 <- table(Trainset.Actual = trainset$Default, m2.predict.train, deparse.level = 2)
table1 <- table(Actual = data1$AHD, m2.predict, deparse.level = 2)
table1
prob.train <- predict(m2, type = 'response')
m2.predict.train <- ifelse(prob.train > threshold1, "Yes", "No")
table1 <- table(trainset.Actual = trainset$Default, m2.predict.train, deparse.level = 2)
# Train-Test split ---------------------------------------------------
set.seed(2)
train <- sample.split(Y = data1$AHD, SplitRatio = 0.7)
trainset <- subset(data1, train == T)
testset <- subset(data1, train == F)
#Logistic Regression
m1 <- glm(AHD ~ . -Ca , family = binomial, data = data1)
summary(m1)
m2 <- glm(AHD ~ . -Ca -RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(m2)
OR.m2 <- exp(coef(m2))
OR.m2
# Output the probability from the logistic function for all cases in the data.
prob <- predict(m2, type = 'response')
# If Threshold = 0.5 ---------------------------------------------
threshold1 <- 0.5
m2.predict <- ifelse(prob > threshold1, "Yes", "No")
table1 <- table(Actual = data1$AHD, m2.predict, deparse.level = 2)
table1
round(prop.table(table1),3)
# Overall Accuracy of the Logistic Regression Model
mean(m2.predict == data1$AHD)
#Train Test Split
#70% trainset
set.seed(2)
train <- sample.split(Y = data1$AHD, SplitRatio = 0.7)
trainset <- subset(data1, train == T)
testset <- subset(data1, train == F)
final_model <- glm(AHD ~ . -Ca -RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
final_model <- glm(AHD ~ . -Ca -RestECG -Age -Chol -Fbs -ExAng -Oldpeak -MaxHR, family = binomial, data = data1)
summary(final_model)
OR <- exp(coef(final_model))
OR
OR.CI <- exp(confint(final_model))
OR.CI
# Confusion Matrix on Trainset
prob.train <- predict(final_model, type = 'response')
final_model.predict.train <- ifelse(prob.train > threshold1, "Yes", "No")
table3 <- table(Trainset.Actual = trainset$Default, final_model.predict.train, deparse.level = 2)
table3 <- table(trainset.Actual = trainset$AHD, final_model.predict.train, deparse.level = 2)
trainset$AHD
final_model.predict.train
final_model.predict.train
final_model.predict.train
train <- sample.split(Y = data1$AHD, SplitRatio = 0.7)
trainset <- subset(data1, train == T)
trainset
View(trainset)
final_model.predict.trainset
View(train)
View(data1)
train <- sample.split(Y = data1$AHD, SplitRatio = 0.7)
length(train)
?sample.split
final_model.predict.trainset
final_model.predict.trainset
final_model.predict.train
table3 <- table(trainset.Actual = trainset$AHD, final_model.predict.train, deparse.level = 2)
data(mtcars)
# Correlation Matrix
cor(mtcars)
round(cor(mtcars), 2)
# Correlation Plot
install.packages("corrplot")
library(corrplot)
corrplot(cor(mtcars), type = "upper")
plot(mtcars$mpg, mtcars$wt)
cor(mtcars$mpg, mtcars$wt)
## -0.8676594
plot(mtcars$mpg, mtcars$hp)
cor(mtcars$mpg, mtcars$hp)
## -0.7761684
plot(mtcars$mpg, mtcars$qsec)
cor(mtcars$mpg, mtcars$qsec)
## 0.418684
plot(mtcars$drat, mtcars$qsec)
cor(mtcars$drat, mtcars$qsec)
## 0.09120476
plot(mtcars$hp, mtcars$cyl)
cor(mtcars$hp, mtcars$cyl)
## 0.8324475
# Outcome Variable: MPG as higher MPG means costs savings on petrol.
# m1 with wt only -------------------------------------------------------------------
m1 <- lm(mpg ~ wt, data = mtcars)
summary(m1) # See the PPT file
plot(x = mtcars$wt, y = mtcars$mpg, main = "Regression Line with wt as sole factor")
abline(m1, col = "red")
identify(x = mtcars$wt, y = mtcars$mpg) # Identify the cases selected in Plot.
par(mfrow = c(2,2))  # Plot 4 charts in one plot - 2 by 2.
plot(m1)  # Plot model 1 diagnostics, See the PPT file
par(mfrow = c(1,1))  # Reset plot options to 1 chart in one plot.
#-------------------------------------------------------------------------------------
# m2 with Wt and Wt^2 ----------------------------------------------------------------
m2 <- lm(mpg ~ wt + I(wt^2), data = mtcars)
summary(m2)
par(mfrow = c(2,2))  # Plot 4 charts in one plot - 2 by 2.
plot(m2)  # Plot model 2 diagnostics
par(mfrow = c(1,1))  # Reset plot options to 1 chart in one plot.
# -------------------------------------------------------------------------------------
# m3 with wt and cyl ----------------------------------------------------------------
str(mtcars$cyl)
mtcars <- mtcars  # create a copy to preserve the orignal dataset.
mtcars$cyl <- factor(mtcars$cyl)
str(mtcars$cyl)  # Check data structure is now factor
levels(mtcars$cyl)
m3 <- lm(mpg ~ wt + cyl, data = mtcars)
summary(m3)  # See the PPT file
par(mfrow = c(2,2))
plot(m3)  # Plot model 3 diagnostics
par(mfrow = c(1,1))
# -----------------------------------------------------------------------------------
# m.full and m4 from backward elimination -------------------------------------------
mtcars$vs <- factor(mtcars$vs)  # vs: V-shaped engine vs. Straight engine
mtcars$am <- factor(mtcars$am)  # am: automatic transmission vs. manual transmission
mtcars$gear <- factor(mtcars$gear)  # gear: number of forward gears
mtcars$carb <- factor(mtcars$carb)  # carb: number of carburetors
m.full <- lm(mpg ~ . , data = mtcars)  # . means all other variables
m4 <- step(m.full)  # Akaike Information Criterion
summary(m4)
par(mfrow = c(2,2)) # Plot 4 charts in one plot - 2 by 2.
plot(m4)  # Plot model 4 diagnostics
par(mfrow = c(1,1)) # Reset plot options to 1 chart in one plot.
# ------------------------------------------------------------------------------------
# VIF --------------------------------------------------------------------------------------------------------------
summary(m.full)  #cyl 4 > cyl 6 < cyl 8
plot(x = mtcars$cyl, y = mtcars$mpg, main ="The higher the cyl, the lower the mpg", xlab = "cyl", ylab = "mpg")
install.packages("car")
library(car)
vif(m.full) # many variables have adjusted GVIF above 2, that for cyl is way above 2
vif(m4)     # adjusted GVIF around or less than 2, coefficients are much more stable
summary(m4)
vif(m3)     # adjusted GVIF below 2
summary(m3)
vif(m2)     # VIF greater than 10, not a serious concern since wt^2 and wt are supposed to be highly correlated
summary(m2)
# Generate a random number sequence that can be reproduced to verify results.
set.seed(2004)
# 70% trainset. Stratify on Y = mpg. Caution: Sample size only 32 in this example.
train <- sample.split(Y = mtcars$mpg, SplitRatio = 0.7)
trainset <- subset(mtcars, train == T)
testset <- subset(mtcars, train == F)
# Checking the distribution of Y is similar in trainset vs testset.
summary(trainset$mpg)
summary(testset$mpg)
# Develop model on trainset
m5 <- lm(mpg ~ wt + cyl, data = trainset)
summary(m5)
residuals(m5)
# Residuals = Error = Actual mpg - Model Predicted mpg
RMSE.m5.train <- sqrt(mean(residuals(m5)^2))  # RMSE on trainset based on m5 model.
summary(abs(residuals(m5)))  # Check Min Abs Error and Max Abs Error.
# Apply model from trainset to predict on testset.
predict.m5.test <- predict(m5, newdata = testset)
# ========================================================================================================
# Purpose:      Demo of Binary Logistic Regression model with multiple X
# Author:       Neumann Chew
# DOC:          25-09-2017
# Topics:       Logistic Regression; Odds Ratios; OR Confidence Intervals.
# Data Source:  default.csv as in ISLR Rpackage
# Packages:     data.table, caTools
#=========================================================================================================
library(data.table)
library(caTools)
default.dt <- fread("default.csv", stringsAsFactors = T)
summary(default.dt)
cor(default.dt$AvgBal, default.dt$Income)
levels(default.dt$Default) # Baseline is Default = "No"
# ========================================================================================================
# Purpose:      Demo of Binary Logistic Regression model with multiple X
# Author:       Neumann Chew
# DOC:          25-09-2017
# Topics:       Logistic Regression; Odds Ratios; OR Confidence Intervals.
# Data Source:  default.csv as in ISLR Rpackage
# Packages:     data.table, caTools
#=========================================================================================================
library(data.table)
library(caTools)
install.packages("car")
# ========================================================================================================
# Purpose:      Demo of Binary Logistic Regression model with multiple X
# Author:       Neumann Chew
# DOC:          25-09-2017
# Topics:       Logistic Regression; Odds Ratios; OR Confidence Intervals.
# Data Source:  default.csv as in ISLR Rpackage
# Packages:     data.table, caTools
#=========================================================================================================
library(data.table)
library(caTools)
default.dt <- fread("default.csv", stringsAsFactors = T)
setwd("~/Desktop/Solutions R/Unit 7 - Logistic Reg")
setwd("~/Desktop/Solutions R/Unit 7 - Logistic Reg")
default.dt <- fread("default.csv", stringsAsFactors = T)
summary(default.dt)
cor(default.dt$AvgBal, default.dt$Income)
levels(default.dt$Default) # Baseline is Default = "No"
m1 <- glm(Default ~ . , family = binomial, data = default.dt)
summary(m1)
m2 <- glm(Default ~ . -Income, family = binomial, data = default.dt)
summary(m2)
OR.m2 <- exp(coef(m2))
OR.m2
OR.CI.m2 <- exp(confint(m2))
OR.CI.m2
# Output the probability from the logistic function for all cases in the data.
prob <- predict(m2, type = 'response')
# If Threshold = 0.5 ---------------------------------------------
threshold1 <- 0.5
m2.predict <- ifelse(prob > threshold1, "Yes", "No")
table1 <- table(Actual = default.dt$Default, m2.predict, deparse.level = 2)
table1
round(prop.table(table1),3)
# Overall Accuracy
mean(m2.predict == default.dt$Default)
# Train-Test split ---------------------------------------------------
set.seed(2)
train <- sample.split(Y = default.dt$Default, SplitRatio = 0.7)
trainset <- subset(default.dt, train == T)
testset <- subset(default.dt, train == F)
m3 <- glm(Default ~ . , family = binomial, data = trainset)
summary(m3)
m4 <- glm(Default ~ . -Income, family = binomial, data = trainset)
summary(m4)
OR <- exp(coef(m4))
OR
OR.CI <- exp(confint(m4))
OR.CI
# Confusion Matrix on Trainset
prob.train <- predict(m4, type = 'response')
m4.predict.train <- ifelse(prob.train > threshold1, "Yes", "No")
table3 <- table(Trainset.Actual = trainset$Default, m4.predict.train, deparse.level = 2)
table3
round(prop.table(table3),3)
# Overall Accuracy
mean(m4.predict.train == trainset$Default)
# Confusion Matrix on Testset
prob.test <- predict(m4, newdata = testset, type = 'response')
m4.predict.test <- ifelse(prob.test > threshold1, "Yes", "No")
table4 <- table(Testset.Actual = testset$Default, m4.predict.test, deparse.level = 2)
table4
round(prop.table(table4), 3)
# Overall Accuracy
mean(m4.predict.test == testset$Default)
options(scipen=100)  # suppress scientific notation by using penalty
summary(default.dt$AvgBal)
sd(default.dt$AvgBal)
default2.dt <- default.dt
default2.dt[, bal500 := AvgBal/500]  # a 1 unit increase in bal500 is a $500 increase in balance
m5 <- glm(Default ~ Gender + bal500, family = binomial, data = default2.dt)
summary(m5)
OR.m5 <- exp(coef(m5))
OR.m5
OR.CI.m5 <- exp(confint(m5))
OR.CI.m5
# Alternative by modifying the OR equation to multiply the coef by the number to be incremented:
exp(500*5.738e-03)
## Whereas the OR for $1 increase in balance is very small [see m2],
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
## Whereas the OR for $1 increase in balance is very small [see m2],
## OR for $500 increase in balance is very big [see m5].
#---------------------------------------------------------------------------------------
# Confusion Matrix on Trainset
prob.train <- predict(m4, type = 'response')
m4.predict.train <- ifelse(prob.train > threshold1, "Yes", "No")
table3 <- table(Trainset.Actual = trainset$Default, m4.predict.train, deparse.level = 2)
m4.predict.train
m4.predict.train
length(train)
length(trainset)
length(testset)
train <- sample.split(Y = default.dt$Default, SplitRatio = 0.7)
trainset <- subset(default.dt, train == T)
testset <- subset(default.dt, train == F)
length(train)
length(trainset)
View(testset)
trainset$Default
m4.predict.train
